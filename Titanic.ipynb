{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "FINAL = True\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "p_id = test_data['PassengerId']\n",
    "data = pd.concat([train_data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('PassengerId', axis=1, inplace=True)\n",
    "survived = data['Survived'].dropna()\n",
    "data['Survived'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(dataframe, name):\n",
    "    dataframe = pd.concat([dataframe, pd.get_dummies(dataframe[name])\n",
    "                           .rename(columns=lambda x: name + str(x))], axis=1)\n",
    "    return dataframe.drop(name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    data['Cabin'].fillna('U0', inplace=True)\n",
    "    data['CabinSection'] = LabelEncoder().fit_transform(data['Cabin'].map(lambda x: x[0]))\n",
    "    \n",
    "    data['CabinDistance'] = data['Cabin'].map(lambda x: x[1:])\n",
    "    data['CabinDistance'] = data['CabinDistance'].map(lambda x: x.split(' ')[0])\n",
    "    data['CabinDistance'].where(data['CabinDistance'] != '', '0', inplace=True)\n",
    "    data['CabinDistance'] = data['CabinDistance'].map(lambda x: int(x))\n",
    "    data['CabinDistance'] = StandardScaler().fit_transform(data['CabinDistance'].values.reshape(-1, 1))\n",
    "\n",
    "    data['Sex'] = LabelEncoder().fit_transform(data['Sex'])\n",
    "\n",
    "    data['Embarked'].fillna('S', inplace=True)\n",
    "    data['Embarked'] = LabelEncoder().fit_transform(data['Embarked'])\n",
    "    \n",
    "    data['Name'] = data['Name'].map(lambda x: x.split(',')[1].split('.')[0])\n",
    "    data['Name'] = LabelEncoder().fit_transform(data['Name'])\n",
    "    \n",
    "    data['Fare'].fillna(-1, inplace=True)\n",
    "    medians = dict()\n",
    "    for pclass in data['Pclass'].unique():\n",
    "        median = data.Fare[(data[\"Fare\"] != -1) & (data['Pclass'] == pclass)].median()\n",
    "        medians[pclass] = median\n",
    "    for index, row in data.iterrows():\n",
    "        if row['Fare'] == -1:\n",
    "            data.loc[index, 'Fare'] = medians[row['Pclass']]\n",
    "    data['Fare'] = StandardScaler().fit_transform(data['Fare'].values.reshape(-1, 1))\n",
    "    #data.drop('Fare', axis=1, inplace=True)\n",
    "    \n",
    "    data['Age'].fillna(-1, inplace=True)\n",
    "    medians = dict()\n",
    "    for title in data['Name'].unique():\n",
    "        median = data.Age[(data[\"Age\"] != -1) & (data['Name'] == title)].median()\n",
    "        medians[title] = median\n",
    "    for index, row in data.iterrows():\n",
    "        if row['Age'] == -1:\n",
    "            data.loc[index, 'Age'] = medians[row['Name']]\n",
    "            \n",
    "    data['Age'] = StandardScaler().fit_transform(data['Age'].values.reshape(-1, 1))\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        ticket = row['Ticket']\n",
    "        sibsp = row['SibSp']\n",
    "        parch = row['Parch']\n",
    "\n",
    "        if sibsp > 0 or parch > 0:\n",
    "            ages = list()\n",
    "            for index2, row2 in data[data['Ticket'] == ticket].iterrows():\n",
    "                ages.append(row2['Age'])\n",
    "            data.loc[index, 'Age2'] = min(ages)\n",
    "\n",
    "        else:\n",
    "            data.loc[index, 'Age2'] = row['Age']\n",
    "            \n",
    "    data['Age2'] = StandardScaler().fit_transform(data['Age2'].values.reshape(-1, 1))\n",
    "    \n",
    "    died_titles = ('Don', 'Rev', 'Capt', 'Jonkheer')\n",
    "    survived_titles = ('Mme', 'Ms', 'Lady', 'Sir', 'Mlle', 'the Countess')\n",
    "    data['Title_Died'] = data['Name'].apply(lambda x: int(x in died_titles))\n",
    "    data['Title_Survived'] = data['Name'].apply(lambda x: int(x in survived_titles))\n",
    "\n",
    "    for title in ('Mr', 'Mrs', 'Miss', 'Master', 'Dr', 'Major', 'Col'):\n",
    "        data['Title_{}'.format(title)] = data['Name'].apply(lambda x: int(x == title))\n",
    "\n",
    "    data.drop('Name', axis=1, inplace=True)\n",
    "    \n",
    "    #data.drop('Name', axis=1, inplace=True)\n",
    "    \n",
    "    #data = one_hot(data, 'CabinSection')\n",
    "    data = one_hot(data, 'Pclass')\n",
    "    #data = one_hot(data, 'Embarked')\n",
    "    \n",
    "    data.drop('Cabin', axis=1, inplace=True)\n",
    "    data.drop('Ticket', axis=1, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sagar\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "processed_data = preprocess_data(data)\n",
    "\n",
    "training_data = processed_data[data['Survived'] != -1]\n",
    "testing_data = processed_data[data['Survived'] == -1]\n",
    "\n",
    "training_data.drop('Survived', axis=1, inplace=True)\n",
    "testing_data.drop('Survived', axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, survived, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491620111731844\n",
      "0.8044692737430168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sagar\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "models = [\n",
    "    #KNeighborsClassifier(3),\n",
    "    #SVC(kernel=\"linear\", C=0.025),\n",
    "    #SVC(gamma=2, C=1),\n",
    "    #DecisionTreeClassifier(max_depth=10),\n",
    "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                       max_depth=10, max_features='sqrt', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=5, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=6,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False),\n",
    "    MLPClassifier(),\n",
    "    #AdaBoostClassifier(),\n",
    "    #GaussianNB(),\n",
    "    #QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sagar\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "if FINAL:\n",
    "\n",
    "    models = [\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        MLPClassifier(),\n",
    "    ]\n",
    "\n",
    "    i=1\n",
    "    for model in models:\n",
    "        model.fit(training_data, survived)\n",
    "        prediction = model.predict(testing_data)\n",
    "        np.savetxt('submission{}.csv'.format(i), prediction, delimiter=\",\")\n",
    "        i += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
